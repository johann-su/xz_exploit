@online{262588213843476WIPXZBackdoora,
  title = {[{{WIP}}] {{XZ Backdoor Analysis}} and Symbol Mapping},
  author = {262588213843476},
  url = {https://gist.github.com/smx-smx/a6112d54777845d389bd7126d6e9f504},
  urldate = {2024-05-29},
  abstract = {[WIP] XZ Backdoor Analysis and symbol mapping. GitHub Gist: instantly share code, notes, and snippets.},
  langid = {english},
  organization = {Gist},
  file = {/Users/johann/Zotero/storage/LYHPKYGT/a6112d54777845d389bd7126d6e9f504.html}
}

@online{262588213843476XzutilsBackdoorSituation,
  title = {Xz-Utils Backdoor Situation ({{CVE-2024-3094}})},
  author = {262588213843476},
  url = {https://gist.github.com/thesamesam/223949d5a074ebc3dce9ee78baad9e27},
  urldate = {2024-05-29},
  abstract = {xz-utils backdoor situation (CVE-2024-3094). GitHub Gist: instantly share code, notes, and snippets.},
  langid = {english},
  organization = {Gist},
  file = {/Users/johann/Zotero/storage/EGU7QUUT/223949d5a074ebc3dce9ee78baad9e27.html}
}

@online{AgnerCPUBlog,
  title = {Agner`s {{CPU}} Blog - {{Gnu}} Support for {{CPU}} Dispatching - Sort Of...},
  url = {https://www.agner.org/optimize/blog/read.php?i=167},
  urldate = {2024-05-30},
  file = {/Users/johann/Zotero/storage/H67BBAYY/read.html}
}

@online{andresfreundtecWasDoingMicrobenchmarking2024,
  type = {Mastodon post},
  title = {I Was Doing Some Micro-Benchmarking at the Time, Needed to Quiesce the System to Reduce Noise. {{Saw}} Sshd Processes Were Using a Surprising…},
  author = {AndresFreundTec, (@AndresFreundTec@mastodon.social)},
  date = {2024-03-29},
  url = {https://mastodon.social/@AndresFreundTec/112180406142695845},
  urldate = {2024-05-29},
  abstract = {I was doing some micro-benchmarking at the time, needed to quiesce the system to reduce noise. Saw sshd processes were using a surprising amount of CPU, despite immediately failing because of wrong usernames etc. Profiled sshd, showing lots of cpu time in liblzma, with perf unable to attribute it to a symbol. Got suspicious. Recalled that I had seen an odd valgrind complaint in automated testing of postgres, a few weeks earlier, after  package updates.Really required a lot of coincidences.},
  langid = {english},
  organization = {Mastodon},
  file = {/Users/johann/Zotero/storage/E2PP9JMG/112180406142695845.html}
}

@online{arjovskyWassersteinGAN2017,
  title = {Wasserstein {{GAN}}},
  author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
  date = {2017-12-06},
  eprint = {1701.07875},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1701.07875},
  urldate = {2024-04-19},
  abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/Users/johann/Zotero/storage/3BLKVYAS/Arjovsky et al. - 2017 - Wasserstein GAN.pdf;/Users/johann/Zotero/storage/J6CC34DW/1701.html}
}

@online{boehsEverythingKnowXZ,
  title = {Everything {{I}} Know about the {{XZ}} Backdoor},
  author = {Boehs, Evan},
  url = {https://boehs.org/node/everything-i-know-about-the-xz-backdoor},
  urldate = {2024-05-29},
  abstract = {Please note: This is being updated in real-time. The intent is to make sense of lots of simultaneous discoveries},
  langid = {english},
  file = {/Users/johann/Zotero/storage/DIW7WZVN/everything-i-know-about-the-xz-backdoor.html}
}

@online{chanEfficientGeometryaware3D2022,
  title = {Efficient {{Geometry-aware 3D Generative Adversarial Networks}}},
  author = {Chan, Eric R. and Lin, Connor Z. and Chan, Matthew A. and Nagano, Koki and Pan, Boxiao and De Mello, Shalini and Gallo, Orazio and Guibas, Leonidas and Tremblay, Jonathan and Khamis, Sameh and Karras, Tero and Wetzstein, Gordon},
  date = {2022-04-27},
  eprint = {2112.07945},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2112.07945},
  url = {http://arxiv.org/abs/2112.07945},
  urldate = {2024-05-05},
  abstract = {Unsupervised generation of high-quality multi-view-consistent images and 3D shapes using only collections of single-view 2D photographs has been a long-standing challenge. Existing 3D GANs are either compute-intensive or make approximations that are not 3D-consistent; the former limits quality and resolution of the generated images and the latter adversely affects multi-view consistency and shape quality. In this work, we improve the computational efficiency and image quality of 3D GANs without overly relying on these approximations. We introduce an expressive hybrid explicit-implicit network architecture that, together with other design choices, synthesizes not only high-resolution multi-view-consistent images in real time but also produces high-quality 3D geometry. By decoupling feature generation and neural rendering, our framework is able to leverage state-of-the-art 2D CNN generators, such as StyleGAN2, and inherit their efficiency and expressiveness. We demonstrate state-of-the-art 3D-aware synthesis with FFHQ and AFHQ Cats, among other experiments.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning},
  file = {/Users/johann/Zotero/storage/QKDA7ELE/Chan et al. - 2022 - Efficient Geometry-aware 3D Generative Adversarial.pdf;/Users/johann/Zotero/storage/E8J84VYC/2112.html}
}

@online{demirPatchBasedImageInpainting2018,
  title = {Patch-{{Based Image Inpainting}} with {{Generative Adversarial Networks}}},
  author = {Demir, Ugur and Unal, Gozde},
  date = {2018-03-20},
  eprint = {1803.07422},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1803.07422},
  url = {http://arxiv.org/abs/1803.07422},
  urldate = {2024-05-03},
  abstract = {Area of image inpainting over relatively large missing regions recently advanced substantially through adaptation of dedicated deep neural networks. However, current network solutions still introduce undesired artifacts and noise to the repaired regions. We present an image inpainting method that is based on the celebrated generative adversarial network (GAN) framework. The proposed PGGAN method includes a discriminator network that combines a global GAN (G-GAN) architecture with a patchGAN approach. PGGAN first shares network layers between G-GAN and patchGAN, then splits paths to produce two adversarial losses that feed the generator network in order to capture both local continuity of image texture and pervasive global features in images. The proposed framework is evaluated extensively, and the results including comparison to recent state-of-the-art demonstrate that it achieves considerable improvements on both visual and quantitative evaluations.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {/Users/johann/Zotero/storage/F4NLYLKN/Demir und Unal - 2018 - Patch-Based Image Inpainting with Generative Adver.pdf;/Users/johann/Zotero/storage/GTZ34THI/1803.html}
}

@video{denzelfarmerDeepDiveXZ2024,
  entrysubtype = {video},
  title = {Deep {{Dive}} into {{XZ Utils Backdoor}} -  {{Columbia Engineering}}, {{Advanced Systems Programming Guest Lecture}}},
  editor = {{Denzel Farmer}},
  editortype = {director},
  date = {2024-04-25},
  url = {https://www.youtube.com/watch?v=Q6ovtLdSbEA},
  urldate = {2024-05-31},
  abstract = {Creative Commons Attribution license (reuse allowed)}
}

@inproceedings{dowrickProcedurallyGeneratedColonoscopy2023,
  title = {Procedurally {{Generated Colonoscopy}} and~{{Laparoscopy Data}} for~{{Improved Model Training Performance}}},
  booktitle = {Data {{Engineering}} in {{Medical Imaging}}},
  author = {Dowrick, Thomas and Chen, Long and Ramalhinho, João and Puyal, Juana González-Bueno and Clarkson, Matthew J.},
  editor = {Bhattarai, Binod and Ali, Sharib and Rau, Anita and Nguyen, Anh and Namburete, Ana and Caramalau, Razvan and Stoyanov, Danail},
  date = {2023},
  pages = {67--77},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-44992-5_7},
  abstract = {The use of synthetic/simulated data can greatly improve model training performance, especially in areas such as image guided surgery, where real training data can be difficult to obtain, or of limited size. Procedural generation of data allows for large datasets to be rapidly generated and automatically labelled, while also randomising relevant parameters within the simulation to provide a wide variation in models and textures used in the scene.},
  isbn = {978-3-031-44992-5},
  langid = {english},
  keywords = {Data Engineering,Image Guided Surgery,notion,Simulation},
  file = {/Users/johann/Zotero/storage/YQGNUJ64/Dowrick et al. - 2023 - Procedurally Generated Colonoscopy and Laparoscopy.pdf}
}

@online{FreeSoftwareNotsoeXZellent,
  title = {Free Software's Not-so-{{eXZellent}} Adventure [{{LWN}}.Net]},
  url = {https://lwn.net/Articles/967866/},
  urldate = {2024-05-30},
  file = {/Users/johann/Zotero/storage/X2YWN469/967866.html}
}

@online{GNUIndirectFunction,
  title = {{{GNU Indirect Function}} and X86 {{ELF ABIs}}},
  url = {https://jasoncc.github.io/gnu_gcc_glibc/gnu-ifunc.html},
  urldate = {2024-05-30},
  abstract = {Random Signals on Some Low-level Stuff.},
  langid = {american},
  organization = {jasoncc.github.io},
  file = {/Users/johann/Zotero/storage/TWKBYP37/gnu-ifunc.html}
}

@online{goodfellowGenerativeAdversarialNetworks2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  date = {2014-06-10},
  eprint = {1406.2661},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1406.2661},
  urldate = {2024-04-09},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/Users/johann/Zotero/storage/IK96PPMK/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf;/Users/johann/Zotero/storage/IZH5PVR2/1406.html}
}

@online{goodinWhatWeKnow2024,
  title = {What We Know about the Xz {{Utils}} Backdoor That Almost Infected the World},
  author = {Goodin, Dan},
  date = {2024-04-01T06:55:22+00:00},
  url = {https://arstechnica.com/security/2024/04/what-we-know-about-the-xz-utils-backdoor-that-almost-infected-the-world/},
  urldate = {2024-05-29},
  abstract = {Malicious updates made to a ubiquitous tool were a few weeks away from going mainstream.},
  langid = {american},
  organization = {Ars Technica}
}

@online{gulrajaniImprovedTrainingWasserstein2017,
  title = {Improved {{Training}} of {{Wasserstein GANs}}},
  author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
  date = {2017-12-25},
  eprint = {1704.00028},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1704.00028},
  url = {http://arxiv.org/abs/1704.00028},
  urldate = {2024-05-03},
  abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/Users/johann/Zotero/storage/3SHWCUC6/Gulrajani et al. - 2017 - Improved Training of Wasserstein GANs.pdf;/Users/johann/Zotero/storage/L49EMBE5/1704.html}
}

@article{guoMedGANAdaptiveGAN2023,
  title = {{{MedGAN}}: {{An}} Adaptive {{GAN}} Approach for Medical Image Generation},
  shorttitle = {{{MedGAN}}},
  author = {Guo, Kehua and Chen, Jie and Qiu, Tian and Guo, Shaojun and Luo, Tao and Chen, Tianyu and Ren, Sheng},
  date = {2023-09-01},
  journaltitle = {Computers in Biology and Medicine},
  shortjournal = {Computers in Biology and Medicine},
  volume = {163},
  pages = {107119},
  issn = {0010-4825},
  doi = {10.1016/j.compbiomed.2023.107119},
  url = {https://www.sciencedirect.com/science/article/pii/S001048252300584X},
  urldate = {2024-04-19},
  abstract = {Generative adversarial networks (GANs) and their variants as an effective method for generating visually appealing images have shown great potential in different medical imaging applications during past decades. However, some issues remain insufficiently investigated: many models still suffer from model collapse, vanishing gradients, and convergence failure. Considering the fact that medical images differ from typical RGB images in terms of complexity and dimensionality, we propose an adaptive generative adversarial network, namely MedGAN, to mitigate these issues. Specifically, we first use Wasserstein loss as a convergence metric to measure the convergence degree of the generator and the discriminator. Then, we adaptively train MedGAN based on this metric. Finally, we generate medical images based on MedGAN and use them to build few-shot medical data learning models for disease classification and lesion localization. On demodicosis, blister, molluscum, and parakeratosis datasets, our experimental results verify the advantages of MedGAN in model convergence, training speed, and visual quality of generated samples. We believe this approach can be generalized to other medical applications and contribute to radiologists’ efforts for disease diagnosis. The source code can be downloaded at https://github.com/geyao-c/MedGAN.},
  keywords = {Deep learning,Generative adversarial network,Image synthesis,Medical imaging,notion},
  file = {/Users/johann/Zotero/storage/SRPE3G8H/main.pdf;/Users/johann/Zotero/storage/94JS5ER3/S001048252300584X.html}
}

@online{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2015-12-10},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1512.03385},
  url = {http://arxiv.org/abs/1512.03385},
  urldate = {2024-05-03},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/johann/Zotero/storage/MCIYGGF9/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;/Users/johann/Zotero/storage/4HAZPU29/1512.html}
}

@online{hongLRMLargeReconstruction2024,
  title = {{{LRM}}: {{Large Reconstruction Model}} for {{Single Image}} to {{3D}}},
  shorttitle = {{{LRM}}},
  author = {Hong, Yicong and Zhang, Kai and Gu, Jiuxiang and Bi, Sai and Zhou, Yang and Liu, Difan and Liu, Feng and Sunkavalli, Kalyan and Bui, Trung and Tan, Hao},
  date = {2024-03-09},
  eprint = {2311.04400},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2311.04400},
  urldate = {2024-04-19},
  abstract = {We propose the first Large Reconstruction Model (LRM) that predicts the 3D model of an object from a single input image within just 5 seconds. In contrast to many previous methods that are trained on small-scale datasets such as ShapeNet in a category-specific fashion, LRM adopts a highly scalable transformer-based architecture with 500 million learnable parameters to directly predict a neural radiance field (NeRF) from the input image. We train our model in an end-to-end manner on massive multi-view data containing around 1 million objects, including both synthetic renderings from Objaverse and real captures from MVImgNet. This combination of a high-capacity model and large-scale training data empowers our model to be highly generalizable and produce high-quality 3D reconstructions from various testing inputs, including real-world in-the-wild captures and images created by generative models. Video demos and interactable 3D meshes can be found on our LRM project webpage: https://yiconghong.me/LRM.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning,notion},
  file = {/Users/johann/Zotero/storage/TZCIVCZ9/Hong et al. - 2024 - LRM Large Reconstruction Model for Single Image t.pdf;/Users/johann/Zotero/storage/HPSW32KT/2311.html}
}

@online{HowXZBackdoor,
  title = {How the {{XZ}} Backdoor Works [{{LWN}}.Net]},
  url = {https://lwn.net/Articles/967192/},
  urldate = {2024-05-30},
  file = {/Users/johann/Zotero/storage/6FRJ7MWU/967192.html}
}

@online{huangMultimodalUnsupervisedImagetoImage2018,
  title = {Multimodal {{Unsupervised Image-to-Image Translation}}},
  author = {Huang, Xun and Liu, Ming-Yu and Belongie, Serge and Kautz, Jan},
  date = {2018-08-14},
  eprint = {1804.04732},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1804.04732},
  urldate = {2024-04-09},
  abstract = {Unsupervised image-to-image translation is an important and challenging problem in computer vision. Given an image in the source domain, the goal is to learn the conditional distribution of corresponding images in the target domain, without seeing any pairs of corresponding images. While this conditional distribution is inherently multimodal, existing approaches make an overly simplified assumption, modeling it as a deterministic one-to-one mapping. As a result, they fail to generate diverse outputs from a given source domain image. To address this limitation, we propose a Multimodal Unsupervised Image-to-image Translation (MUNIT) framework. We assume that the image representation can be decomposed into a content code that is domain-invariant, and a style code that captures domain-specific properties. To translate an image to another domain, we recombine its content code with a random style code sampled from the style space of the target domain. We analyze the proposed framework and establish several theoretical results. Extensive experiments with comparisons to the state-of-the-art approaches further demonstrates the advantage of the proposed framework. Moreover, our framework allows users to control the style of translation outputs by providing an example style image. Code and pretrained models are available at https://github.com/nvlabs/MUNIT},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/Users/johann/Zotero/storage/K8G47PMD/Huang et al. - 2018 - Multimodal Unsupervised Image-to-Image Translation.pdf;/Users/johann/Zotero/storage/MTGMYB2U/1804.html}
}

@online{isolaImagetoImageTranslationConditional2018,
  title = {Image-to-{{Image Translation}} with {{Conditional Adversarial Networks}}},
  author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
  date = {2018-11-26},
  eprint = {1611.07004},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1611.07004},
  url = {http://arxiv.org/abs/1611.07004},
  urldate = {2024-05-03},
  abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {/Users/johann/Zotero/storage/7JBG83GJ/Isola et al. - 2018 - Image-to-Image Translation with Conditional Advers.pdf;/Users/johann/Zotero/storage/CSZJLN3S/1611.html}
}

@online{JiaT75JiaTan,
  title = {{{JiaT75}} ({{Jia Tan}}) / {{December}} 2020},
  url = {https://github.com/JiaT75?tab=overview&from=2020-12-01&to=2020-12-31},
  urldate = {2024-05-30},
  file = {/Users/johann/Zotero/storage/693DCTIH/JiaT75.html}
}

@inproceedings{jungConditionalGANAttentionBased2021,
  title = {Conditional {{GAN}} with an {{Attention-Based Generator}} and a {{3D Discriminator}} for {{3D Medical Image Generation}}},
  author = {Jung, Euijin and Luna, Miguel and Park, Sang Hyun},
  editor = {family=Bruijne, given=Marleen, prefix=de, useprefix=true and Cattin, Philippe C. and Cotin, Stéphane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
  date = {2021},
  pages = {318--328},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-87231-1_31},
  abstract = {Conditional Generative Adversarial Networks (cGANs) are a set of methods able to synthesize images that match a given condition. However, existing models designed for natural images are impractical to generate high-quality 3D medical images due to enormous computation. To address this issue, most cGAN models used in the medical field process either 2D slices or small 3D crops and join them together in subsequent steps to reconstruct the full-size 3D image. However, these approaches often cause spatial inconsistencies in adjacent slices or crops, and the changes specified by the target condition may not consider the 3D image as a whole. To address these problems, we propose a novel cGAN that can synthesize high-quality 3D MR images at different stages of the Alzheimer’s disease (AD). First, our method generates a sequence of 2D slices using an attention-based 2D generator with a disease condition for efficient transformations depending on brain regions. Then, consistency in 3D space is enforced by the use of a set of 2D and 3D discriminators. Moreover, we propose an adaptive identity loss based on the attention scores to properly transform features relevant to the target condition. Our experiments show that the proposed method can generate smooth and realistic 3D images at different stages of AD, and the image change with respect to the condition is better than the images generated by existing GAN-based methods.},
  isbn = {978-3-030-87231-1},
  langid = {english},
  keywords = {notion},
  file = {/Users/johann/Zotero/storage/CS92I6P5/Jung et al. - 2021 - Conditional GAN with an Attention-Based Generator and a 3D Discriminator for 3D Medical Image Generation.pdf}
}

@online{leeSPIGANPrivilegedAdversarial2019,
  title = {{{SPIGAN}}: {{Privileged Adversarial Learning}} from {{Simulation}}},
  shorttitle = {{{SPIGAN}}},
  author = {Lee, Kuan-Hui and Ros, German and Li, Jie and Gaidon, Adrien},
  date = {2019-02-18},
  eprint = {1810.03756},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1810.03756},
  urldate = {2024-04-09},
  abstract = {Deep Learning for Computer Vision depends mainly on the source of supervision.Photo-realistic simulators can generate large-scale automatically labeled syntheticdata, but introduce a domain gap negatively impacting performance. We propose anew unsupervised domain adaptation algorithm, called SPIGAN, relying on Sim-ulator Privileged Information (PI) and Generative Adversarial Networks (GAN).We use internal data from the simulator as PI during the training of a target tasknetwork. We experimentally evaluate our approach on semantic segmentation. Wetrain the networks on real-world Cityscapes and Vistas datasets, using only unla-beled real-world images and synthetic labeled data with z-buffer (depth) PI fromthe SYNTHIA dataset. Our method improves over no adaptation and state-of-the-art unsupervised domain adaptation techniques.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {/Users/johann/Zotero/storage/2SU7M4FN/Lee et al. - 2019 - SPIGAN Privileged Adversarial Learning from Simul.pdf;/Users/johann/Zotero/storage/KYFLZNU9/1810.html}
}

@online{mirzaConditionalGenerativeAdversarial2014,
  title = {Conditional {{Generative Adversarial Nets}}},
  author = {Mirza, Mehdi and Osindero, Simon},
  date = {2014-11-06},
  eprint = {1411.1784},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1411.1784},
  url = {http://arxiv.org/abs/1411.1784},
  urldate = {2024-05-03},
  abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/Users/johann/Zotero/storage/MHQ2TUML/Mirza und Osindero - 2014 - Conditional Generative Adversarial Nets.pdf;/Users/johann/Zotero/storage/LHK7655U/1411.html}
}

@online{NVDCVE20243094,
  title = {{{NVD}} - {{CVE-2024-3094}}},
  url = {https://nvd.nist.gov/vuln/detail/CVE-2024-3094},
  urldate = {2024-05-29}
}

@online{officefirst1279IfItAintBrokeDontFixIt2024,
  type = {Reddit Post},
  title = {{{ifItAintBrokeDontFixIt}}},
  author = {OfficeFirst1279},
  date = {2024-04-10T05:31:41},
  url = {www.reddit.com/r/ProgrammerHumor/comments/1c0e03q/ifitaintbrokedontfixit/},
  urldate = {2024-05-29},
  organization = {r/ProgrammerHumor},
  file = {/Users/johann/Zotero/storage/WEBTZHD9/ifitaintbrokedontfixit.html}
}

@online{OsssecurityBackdoorUpstream,
  title = {Oss-Security - Backdoor in Upstream Xz/Liblzma Leading to Ssh Server Compromise},
  url = {https://www.openwall.com/lists/oss-security/2024/03/29/4},
  urldate = {2024-05-29},
  file = {/Users/johann/Zotero/storage/3NHLRRYX/4.html}
}

@online{parkContrastiveLearningUnpaired2020,
  title = {Contrastive {{Learning}} for {{Unpaired Image-to-Image Translation}}},
  author = {Park, Taesung and Efros, Alexei A. and Zhang, Richard and Zhu, Jun-Yan},
  date = {2020-08-20},
  eprint = {2007.15651},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2007.15651},
  urldate = {2024-04-09},
  abstract = {In image-to-image translation, each patch in the output should reflect the content of the corresponding patch in the input, independent of domain. We propose a straightforward method for doing so -- maximizing mutual information between the two, using a framework based on contrastive learning. The method encourages two elements (corresponding patches) to map to a similar point in a learned feature space, relative to other elements (other patches) in the dataset, referred to as negatives. We explore several critical design choices for making contrastive learning effective in the image synthesis setting. Notably, we use a multilayer, patch-based approach, rather than operate on entire images. Furthermore, we draw negatives from within the input image itself, rather than from the rest of the dataset. We demonstrate that our framework enables one-sided translation in the unpaired image-to-image translation setting, while improving quality and reducing training time. In addition, our method can even be extended to the training setting where each "domain" is only a single image.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notion},
  file = {/Users/johann/Zotero/storage/3TKQS69X/Park et al. - 2020 - Contrastive Learning for Unpaired Image-to-Image T.pdf;/Users/johann/Zotero/storage/IDFMF637/2007.html}
}

@online{pfeifferGeneratingLargeLabeled2019,
  title = {Generating Large Labeled Data Sets for Laparoscopic Image Processing Tasks Using Unpaired Image-to-Image Translation},
  author = {Pfeiffer, Micha and Funke, Isabel and Robu, Maria R. and Bodenstedt, Sebastian and Strenger, Leon and Engelhardt, Sandy and Roß, Tobias and Clarkson, Matthew J. and Gurusamy, Kurinchi and Davidson, Brian R. and Maier-Hein, Lena and Riediger, Carina and Welsch, Thilo and Weitz, Jürgen and Speidel, Stefanie},
  date = {2019-07-05},
  eprint = {1907.02882},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1907.02882},
  urldate = {2024-04-09},
  abstract = {In the medical domain, the lack of large training data sets and benchmarks is often a limiting factor for training deep neural networks. In contrast to expensive manual labeling, computer simulations can generate large and fully labeled data sets with a minimum of manual effort. However, models that are trained on simulated data usually do not translate well to real scenarios. To bridge the domain gap between simulated and real laparoscopic images, we exploit recent advances in unpaired image-to-image translation. We extent an image-to-image translation method to generate a diverse multitude of realistically looking synthetic images based on images from a simple laparoscopy simulation. By incorporating means to ensure that the image content is preserved during the translation process, we ensure that the labels given for the simulated images remain valid for their realistically looking translations. This way, we are able to generate a large, fully labeled synthetic data set of laparoscopic images with realistic appearance. We show that this data set can be used to train models for the task of liver segmentation of laparoscopic images. We achieve average dice scores of up to 0.89 in some patients without manually labeling a single laparoscopic image and show that using our synthetic data to pre-train models can greatly improve their performance. The synthetic data set will be made publicly available, fully labeled with segmentation maps, depth maps, normal maps, and positions of tools and camera (http://opencas.dkfz.de/image2image).},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/Users/johann/Zotero/storage/WA8UI9H3/Pfeiffer et al. - 2019 - Generating large labeled data sets for laparoscopi.pdf;/Users/johann/Zotero/storage/BFI3STT8/1907.html}
}

@online{PuttingXzBackdoor2024,
  title = {Putting an Xz {{Backdoor Payload}} in a {{Valid RSA Key}}},
  date = {2024-04-03T17:55:00+01:00},
  url = {https://rya.nc/xz-valid-n.html},
  urldate = {2024-05-30},
  abstract = {Last week, a backdoor was discovered in xz-utils. The backdoor processes commands sent using RSA public keys as a covert channel. In order to prevent anyone else from using the backdoor, the threat…},
  langid = {english},
  organization = {rya.nc},
  file = {/Users/johann/Zotero/storage/SXB3C7HZ/xz-valid-n.html}
}

@online{radfordUnsupervisedRepresentationLearning2016,
  title = {Unsupervised {{Representation Learning}} with {{Deep Convolutional Generative Adversarial Networks}}},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  date = {2016-01-07},
  eprint = {1511.06434},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1511.06434},
  url = {http://arxiv.org/abs/1511.06434},
  urldate = {2024-05-03},
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/johann/Zotero/storage/H54KCBID/Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.html}
}

@online{radfordUnsupervisedRepresentationLearning2016a,
  title = {Unsupervised {{Representation Learning}} with {{Deep Convolutional Generative Adversarial Networks}}},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  date = {2016-01-07},
  eprint = {1511.06434},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1511.06434},
  urldate = {2024-05-03},
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notion},
  file = {/Users/johann/Zotero/storage/2ZXMM67L/Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf}
}

@online{radfordUnsupervisedRepresentationLearning2016b,
  title = {Unsupervised {{Representation Learning}} with {{Deep Convolutional Generative Adversarial Networks}}},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  date = {2016-01-07},
  eprint = {1511.06434},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1511.06434},
  urldate = {2024-05-03},
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notion},
  file = {/Users/johann/Zotero/storage/GG5VXI9T/Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf}
}

@online{radfordUnsupervisedRepresentationLearning2016c,
  title = {Unsupervised {{Representation Learning}} with {{Deep Convolutional Generative Adversarial Networks}}},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  date = {2016-01-07},
  eprint = {1511.06434},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1511.06434},
  url = {http://arxiv.org/abs/1511.06434},
  urldate = {2024-05-03},
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notion},
  file = {/Users/johann/Zotero/storage/ULUEZIT4/Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf;/Users/johann/Zotero/storage/ACJAE6J8/1511.html}
}

@online{radfordUnsupervisedRepresentationLearning2016d,
  title = {Unsupervised {{Representation Learning}} with {{Deep Convolutional Generative Adversarial Networks}}},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  date = {2016-01-07},
  eprint = {1511.06434},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1511.06434},
  url = {http://arxiv.org/abs/1511.06434},
  urldate = {2024-05-03},
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,notion},
  file = {/Users/johann/Zotero/storage/BFC7MLYE/Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf;/Users/johann/Zotero/storage/86PCGXYI/1511.html}
}

@online{ResearchRscXz,
  title = {Research!Rsc: {{The}} Xz Attack Shell Script},
  url = {https://research.swtch.com/xz-script},
  urldate = {2024-05-30},
  file = {/Users/johann/Zotero/storage/2VBJ2RNX/xz-script.html}
}

@online{ReverseEngineeringXZ2024,
  title = {Reverse Engineering the {{XZ}} Backdoor},
  date = {2024-04-23T18:48:00+00:00},
  url = {https://clairelevin.github.io/malware/2024/04/23/xz.html},
  urldate = {2024-05-31},
  abstract = {An in-depth look at the backdoor in XZ},
  langid = {english},
  organization = {Claire Levin},
  file = {/Users/johann/Zotero/storage/TYPQA4CR/xz.html}
}

@online{rheaXZBackdoorTimes2024,
  type = {Substack newsletter},
  title = {{{XZ Backdoor}}: {{Times}}, Damned Times, and Scams},
  shorttitle = {{{XZ Backdoor}}},
  author = {Rhea},
  date = {2024-03-30},
  url = {https://rheaeve.substack.com/p/xz-backdoor-times-damned-times-and},
  urldate = {2024-05-30},
  abstract = {Some timezone observations on the recently discovered backdoor hidden in an xz tarball.},
  organization = {Rhea's Substack},
  file = {/Users/johann/Zotero/storage/DP4LKC58/xz-backdoor-times-damned-times-and.html}
}

@online{rocciaLevelSophisticationXZ2024,
  type = {Mastodon post},
  title = {The Level of Sophistication of the {{XZ}} Attack Is Very Impressive! {{I}} Tried to Make Sense of the Analysis in a Single Page (Which Was Quite…},
  author = {Roccia, Thomas (@fr0gger@infosec.exchange)},
  date = {2024-03-31},
  url = {https://infosec.exchangehttps://infosec.exchange/@fr0gger/112189232773640259},
  urldate = {2024-05-30},
  abstract = {The level of sophistication of the XZ attack is very impressive! I tried to make sense of the analysis in a single page (which was quite complicated)! I hope it helps to make sense of the information out there. Please treat the information "as is" while the analysis progresses!  \#infosec \#xz},
  langid = {english},
  organization = {Mastodon},
  file = {/Users/johann/Zotero/storage/MQK4KNNA/112189232773640259.html}
}

@online{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  date = {2015-05-18},
  eprint = {1505.04597},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1505.04597},
  url = {http://arxiv.org/abs/1505.04597},
  urldate = {2024-05-03},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/johann/Zotero/storage/3F4PYA4P/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf;/Users/johann/Zotero/storage/MNIDN5Z5/1505.html}
}

@online{sea_ad_8524XzExploitInANutshell2024,
  type = {Reddit Post},
  title = {{{xzExploitInANutshell}}},
  author = {Sea\_Ad\_8524},
  date = {2024-04-03T09:44:31},
  url = {www.reddit.com/r/ProgrammerHumor/comments/1buoix0/xzexploitinanutshell/},
  urldate = {2024-05-29},
  organization = {r/ProgrammerHumor},
  file = {/Users/johann/Zotero/storage/LMDLMHXC/xzexploitinanutshell.html}
}

@video{seytonicXZBackdoorTimeline2024,
  entrysubtype = {video},
  title = {{{XZ Backdoor}}: {{Timeline}} and {{Overview}}},
  shorttitle = {{{XZ Backdoor}}},
  editor = {{Seytonic}},
  editortype = {director},
  date = {2024-04-05},
  url = {https://www.youtube.com/watch?app=desktop&v=MllrK4XSJxc},
  urldate = {2024-05-29},
  abstract = {Sources: https://research.swtch.com/xz-timeline https://www.openwall.com/lists/oss-se... https://bsky.app/profile/filippo.abys... https://arstechnica.com/security/2024... =============================================== My Website: https://www.seytonic.com/ Follow me on TWTR: ~~/~seytonic~~ Follow me on INSTA: ~~/~jhonti~~ ===============================================}
}

@online{shiZero123SingleImage2023,
  title = {Zero123++: A {{Single Image}} to {{Consistent Multi-view Diffusion Base Model}}},
  shorttitle = {Zero123++},
  author = {Shi, Ruoxi and Chen, Hansheng and Zhang, Zhuoyang and Liu, Minghua and Xu, Chao and Wei, Xinyue and Chen, Linghao and Zeng, Chong and Su, Hao},
  date = {2023-10-23},
  eprint = {2310.15110},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2310.15110},
  urldate = {2024-04-21},
  abstract = {We report Zero123++, an image-conditioned diffusion model for generating 3D-consistent multi-view images from a single input view. To take full advantage of pretrained 2D generative priors, we develop various conditioning and training schemes to minimize the effort of finetuning from off-the-shelf image diffusion models such as Stable Diffusion. Zero123++ excels in producing high-quality, consistent multi-view images from a single image, overcoming common issues like texture degradation and geometric misalignment. Furthermore, we showcase the feasibility of training a ControlNet on Zero123++ for enhanced control over the generation process. The code is available at https://github.com/SUDO-AI-3D/zero123plus.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,notion},
  file = {/Users/johann/Zotero/storage/7CADIWHW/Shi et al. - 2023 - Zero123++ a Single Image to Consistent Multi-view.pdf;/Users/johann/Zotero/storage/TGBNP4I2/Shi et al. - 2023 - Zero123++ a Single Image to Consistent Multi-view.html}
}

@online{StructureIlluminationConstrained,
  title = {Structure and {{Illumination Constrained GAN}} for {{Medical Image Enhancement}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9503421},
  urldate = {2024-04-12},
  keywords = {notion}
}

@online{teamAnalysisXzutilsBackdoor2024,
  title = {Analysis of the Xz-Utils Backdoor Code},
  author = {{team}, Knownsec 404},
  date = {2024-04-29T08:23:19},
  url = {https://medium.com/@knownsec404team/analysis-of-the-xz-utils-backdoor-code-d2d5316ac43f},
  urldate = {2024-05-31},
  abstract = {Author: 0x7F @ Knownsec 404 Team Chinese version: https://paper.seebug.org/3157/},
  langid = {english},
  organization = {Medium},
  file = {/Users/johann/Zotero/storage/9ABDEUK5/analysis-of-the-xz-utils-backdoor-code-d2d5316ac43f.html}
}

@online{the_wolfieeWompWomp2024,
  type = {Reddit Post},
  title = {{{wompWomp}}},
  author = {The\_Wolfiee},
  date = {2024-04-05T13:51:44},
  url = {www.reddit.com/r/ProgrammerHumor/comments/1bwikxc/wompwomp/},
  urldate = {2024-05-29},
  organization = {r/ProgrammerHumor},
  file = {/Users/johann/Zotero/storage/QRTDQG5X/wompwomp.html}
}

@video{theprimetimeXzExploitWILD2024,
  entrysubtype = {video},
  title = {Xz {{Exploit Is WILD}} - {{Must See Bash Part}}},
  editor = {{ThePrimeTime}},
  editortype = {director},
  date = {2024-04-02},
  url = {https://www.youtube.com/watch?app=desktop&v=LaRKIwpGPTU&themeRefresh},
  urldate = {2024-05-29},
  abstract = {Recorded live on twitch, GET IN  Article https://gynvael.coldwind.pl/?lang=en\&... Guest ~~~/~lowlevellearning~~  ~~/~lowlevellearning~~ ~~/~lowleveltweets~~ My Stream ~~/~theprimeagen~~ Best Way To Support Me Become a backend engineer.  Its my favorite site https://boot.dev/?promo=PRIMEYT This is also the best way to support me is to support yourself becoming a better backend engineer.   MY MAIN YT CHANNEL: Has well edited engineering videos ~~~/~theprimeagen~~ Discord ~~/~discord~~ Have something for me to read or react to?: ~~/~theprimeagenreact~~ Kinesis Advantage 360: https://bit.ly/Prime-Kinesis Hey I am sponsored by Turso, an edge database.  I think they are pretty neet.  Give them a try for free and if you want you can get a decent amount off (the free tier is the best (better than planetscale or any other)) https://turso.tech/deeznuts}
}

@online{venkateshExploringSemanticConsistency2024,
  title = {Exploring {{Semantic Consistency}} in {{Unpaired Image Translation}} to {{Generate Data}} for {{Surgical Applications}}},
  author = {Venkatesh, Danush Kumar and Rivoir, Dominik and Pfeiffer, Micha and Kolbinger, Fiona and Distler, Marius and Weitz, Jürgen and Speidel, Stefanie},
  date = {2024-02-21},
  eprint = {2309.03048},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2309.03048},
  urldate = {2024-04-09},
  abstract = {In surgical computer vision applications, obtaining labeled training data is challenging due to data-privacy concerns and the need for expert annotation. Unpaired image-to-image translation techniques have been explored to automatically generate large annotated datasets by translating synthetic images to the realistic domain. However, preserving the structure and semantic consistency between the input and translated images presents significant challenges, mainly when there is a distributional mismatch in the semantic characteristics of the domains. This study empirically investigates unpaired image translation methods for generating suitable data in surgical applications, explicitly focusing on semantic consistency. We extensively evaluate various state-of-the-art image translation models on two challenging surgical datasets and downstream semantic segmentation tasks. We find that a simple combination of structural-similarity loss and contrastive learning yields the most promising results. Quantitatively, we show that the data generated with this approach yields higher semantic consistency and can be used more effectively as training data.The code is available at https://gitlab.com/nct\_tso\_public/constructs.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {/Users/johann/Zotero/storage/H2ZJA35R/Venkatesh et al. - 2024 - Exploring Semantic Consistency in Unpaired Image T.pdf;/Users/johann/Zotero/storage/56ULH2UB/2309.html}
}

@online{wangPixel2MeshGenerating3D2018,
  title = {{{Pixel2Mesh}}: {{Generating 3D Mesh Models}} from {{Single RGB Images}}},
  shorttitle = {{{Pixel2Mesh}}},
  author = {Wang, Nanyang and Zhang, Yinda and Li, Zhuwen and Fu, Yanwei and Liu, Wei and Jiang, Yu-Gang},
  date = {2018-08-03},
  eprint = {1804.01654},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1804.01654},
  urldate = {2024-04-19},
  abstract = {We propose an end-to-end deep learning architecture that produces a 3D shape in triangular mesh from a single color image. Limited by the nature of deep neural network, previous methods usually represent a 3D shape in volume or point cloud, and it is non-trivial to convert them to the more ready-to-use mesh model. Unlike the existing methods, our network represents 3D mesh in a graph-based convolutional neural network and produces correct geometry by progressively deforming an ellipsoid, leveraging perceptual features extracted from the input image. We adopt a coarse-to-fine strategy to make the whole deformation procedure stable, and define various of mesh related losses to capture properties of different levels to guarantee visually appealing and physically accurate 3D geometry. Extensive experiments show that our method not only qualitatively produces mesh model with better details, but also achieves higher 3D shape estimation accuracy compared to the state-of-the-art.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {/Users/johann/Zotero/storage/7742RTUU/Wang et al. - 2018 - Pixel2Mesh Generating 3D Mesh Models from Single .pdf;/Users/johann/Zotero/storage/JPFGU7UB/1804.html}
}

@online{WatchingXzUnfold2024,
  title = {Watching Xz Unfold from Afar},
  date = {2024-03-31T02:30:39},
  url = {https://connortumbleson.com/2024/03/31/watching-xz-unfold-from-afar/},
  urldate = {2024-05-29},
  abstract = {xz/liblzma is discovered to have some malicious code and the story began years earlier.},
  langid = {english},
  organization = {Connor Tumbleson},
  file = {/Users/johann/Zotero/storage/7RACR2NR/watching-xz-unfold-from-afar.html}
}

@software{weemsAmlweemsXzbot2024,
  title = {Amlweems/Xzbot},
  author = {Weems, Anthony},
  date = {2024-05-29T14:58:48Z},
  origdate = {2024-04-01T14:28:09Z},
  url = {https://github.com/amlweems/xzbot},
  urldate = {2024-05-29},
  abstract = {notes, honeypot, and exploit demo for the xz backdoor (CVE-2024-3094)}
}

@online{werviceUpdatesAreAmazing2024,
  type = {Reddit Post},
  title = {{{updatesAreAmazing}}},
  author = {Wervice},
  date = {2024-04-05T22:01:27},
  url = {www.reddit.com/r/ProgrammerHumor/comments/1bwui3j/updatesareamazing/},
  urldate = {2024-05-29},
  organization = {r/ProgrammerHumor},
  file = {/Users/johann/Zotero/storage/RD9XHSUZ/updatesareamazing.html}
}

@online{WeShouldAll2021,
  type = {Reddit Post},
  title = {We Should All Set aside £2 a Week to Help Keep Projects like This Afloat},
  date = {2021-01-22T08:46:30},
  url = {www.reddit.com/r/ProgrammerHumor/comments/l2jonu/we_should_all_set_aside_2_a_week_to_help_keep/},
  urldate = {2024-05-29},
  organization = {r/ProgrammerHumor},
  file = {/Users/johann/Zotero/storage/963KSLLB/we_should_all_set_aside_2_a_week_to_help_keep.html}
}

@online{xuInstantMeshEfficient3D2024,
  title = {{{InstantMesh}}: {{Efficient 3D Mesh Generation}} from a {{Single Image}} with {{Sparse-view Large Reconstruction Models}}},
  shorttitle = {{{InstantMesh}}},
  author = {Xu, Jiale and Cheng, Weihao and Gao, Yiming and Wang, Xintao and Gao, Shenghua and Shan, Ying},
  date = {2024-04-14},
  eprint = {2404.07191},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2404.07191},
  urldate = {2024-04-19},
  abstract = {We present InstantMesh, a feed-forward framework for instant 3D mesh generation from a single image, featuring state-of-the-art generation quality and significant training scalability. By synergizing the strengths of an off-the-shelf multiview diffusion model and a sparse-view reconstruction model based on the LRM architecture, InstantMesh is able to create diverse 3D assets within 10 seconds. To enhance the training efficiency and exploit more geometric supervisions, e.g, depths and normals, we integrate a differentiable iso-surface extraction module into our framework and directly optimize on the mesh representation. Experimental results on public datasets demonstrate that InstantMesh significantly outperforms other latest image-to-3D baselines, both qualitatively and quantitatively. We release all the code, weights, and demo of InstantMesh, with the intention that it can make substantial contributions to the community of 3D generative AI and empower both researchers and content creators.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {/Users/johann/Zotero/storage/3CDM6EXL/Xu et al. - 2024 - InstantMesh Efficient 3D Mesh Generation from a S.pdf;/Users/johann/Zotero/storage/BQ36EAZX/2404.html}
}

@online{XZBackdoorTimeline,
  title = {{{XZ Backdoor}}: {{Timeline}} and {{Overview}} - {{YouTube}}},
  url = {https://www.youtube.com/watch?app=desktop&v=MllrK4XSJxc},
  urldate = {2024-05-29},
  file = {/Users/johann/Zotero/storage/SW98SYXG/watch.html}
}

@online{Xzdevel,
  title = {Xz-Devel},
  url = {https://www.mail-archive.com/xz-devel@tukaani.org/},
  urldate = {2024-05-29},
  file = {/Users/johann/Zotero/storage/7JYDLD8S/xz-devel@tukaani.org.html}
}

@online{XzLiblzmaBashstage,
  title = {Xz/Liblzma: {{Bash-stage Obfuscation Explained}} - Gynvael.Coldwind//vx.Log},
  url = {https://gynvael.coldwind.pl/?lang=en&id=782},
  urldate = {2024-05-29},
  file = {/Users/johann/Zotero/storage/XXCPZZ5D/gynvael.coldwind.pl.html}
}

@online{zhuUnpairedImagetoImageTranslation2020,
  title = {Unpaired {{Image-to-Image Translation}} Using {{Cycle-Consistent Adversarial Networks}}},
  author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  date = {2020-08-24},
  eprint = {1703.10593},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1703.10593},
  urldate = {2024-04-09},
  abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G: X \textbackslash rightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F: Y \textbackslash rightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X)) \textbackslash approx X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,notion},
  file = {/Users/johann/Zotero/storage/GKQFMBQ5/Zhu et al. - 2020 - Unpaired Image-to-Image Translation using Cycle-Co.pdf;/Users/johann/Zotero/storage/HN5G3DHZ/1703.html}
}
